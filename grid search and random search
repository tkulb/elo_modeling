#GRID SEARCH
start = timeit.default_timer()

from sklearn.model_selection import GridSearchCV

rf_clf = RandomForestClassifier()
parameters = {
            "n_estimators": [15,20]
             ,"criterion": ["gini"]
#              ,"max_features": ["auto"]
#              ,"max_depth": ["None"]
#              ,"min_samples_split": [2,5,10]
#              ,"min_samples_leaf": [3,6,10]
#              "n_estimators": [5,4]
#              ,"criterion": ["gini"]
#              ,"max_features": ["auto"]
#              ,"max_depth": [2]
             ,"min_samples_split": [2]
             ,"min_samples_leaf": [1]
             ,"bootstrap": ['True']
             }

grid_cv = GridSearchCV(rf_clf,parameters,scoring = make_scorer(accuracy_score))

grid_cv.fit(X_training,y_training)
grid_cv.best_estimator_
print("Best Parameters:")
print(grid_cv.best_params_)
# print(grid_cv.cv_results_)

print("Results:")
means = grid_cv.cv_results_['mean_test_score']
stds = grid_cv.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, grid_cv.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r"
            % (mean, std * 2, params))

#Fit best parameters and check final RMSE on validation
rf_clf_best = grid_cv.best_estimator_
rf_clf_best.fit(X_training,y_training)

valid_predictions = rf_clf_best.predict(X_valid)

#calc final accuracy, could we record an accuracy for each maybe?
y_valid_trans=lab_enc.inverse_transform(y_valid)

valid_predictions = rf_clf_best.predict(X_valid)
valid_pred_df = lab_enc.inverse_transform(valid_predictions)
valid_pred_df_trans = pd.DataFrame(valid_pred_df,columns=["target"])

delta = valid_pred_df - y_valid_trans
rmse = np.sqrt(sum(np.square(delta)))
print("Best RMSE:", round(rmse,2))

end = timeit.default_timer()
print("total time:",round(end-start,2),"seconds. ",round((end-start)/60,2),"minutes.")



#RANDOM SEARCH
start = timeit.default_timer()

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

rf_clf = RandomForestClassifier()
parameters = {
            "n_estimators": randint(12,20)
#              ,"criterion": ["gini","entropy"]
#              ,"max_features": ["auto","sqrt","log2"]
#              ,"max_depth": [2,5,10]
#              ,"min_samples_split": [2,5,10]
#              ,"min_samples_leaf": [3,6,10]
#              "n_estimators": [5,4]
             ,"criterion": ["gini"]
             ,"max_features": ["auto"]
#              ,"max_depth": [2]
             ,"min_samples_split": [2]
             ,"min_samples_leaf": randint(1,3)
             }

n_iter_search = 6
random_search = RandomizedSearchCV(rf_clf,parameters,scoring = make_scorer(accuracy_score),n_iter=n_iter_search)
random_search.fit(X_training,y_training)
random_search.best_estimator_
print(random_search.best_params_)

means = random_search.cv_results_['mean_test_score']
stds = random_search.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, random_search.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r"
            % (mean, std * 2, params))

#Fit best parameters and check final RMSE on validation
rf_clf_best = random_search.best_estimator_
rf_clf_best.fit(X_training,y_training)

valid_predictions = rf_clf_best.predict(X_valid)

#calc final accuracy, could we record an accuracy for each maybe?
y_valid_trans=lab_enc.inverse_transform(y_valid)

valid_predictions = rf_clf_best.predict(X_valid)
valid_pred_df = lab_enc.inverse_transform(valid_predictions)
valid_pred_df_trans = pd.DataFrame(valid_pred_df,columns=["target"])

delta = valid_pred_df_trans["target"] - y_valid_trans
rmse = np.sqrt(sum(np.square(delta)))
print("Ensemble RMSE:", round(rmse,2))

end = timeit.default_timer()
print("total time:",round(end-start,2),"seconds. ",round((end-start)/60,2),"minutes.")
